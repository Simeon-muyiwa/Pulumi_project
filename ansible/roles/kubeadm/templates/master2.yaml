---
- name: Configure Kubernetes Master Node
  hosts: k8_master
  become: yes
  vars_files:
    - aws.config
  vars:
    kubectl_version: "1.29.*"
    api_server_port: 6443
    etcd_client_port: 2379
    etcd_server_port: 2380
    kubelet_port: 10250
    kube_scheduler_port: 10259
    kube_controller_manager_port: 10257

  tasks:
    # Phase 1: Infrastructure Preparation
    - name: Verify system requirements
      assert:
        that:
          - ansible_processor_vcpus >= 2
          - ansible_memtotal_mb >= 4096
          - ansible_os_family == 'Debian'
        msg: "Insufficient resources or unsupported OS"

    # Phase 2: Package Installation
    - name: Install Kubernetes binaries
      apt:
        name:
          - kubelet={{ kubectl_version }}
          - kubeadm={{ kubectl_version }}
          - kubectl={{ kubectl_version }}
          - jq
          - awscli
        state: present
        update_cache: yes

    # Phase 3: Configuration Generation
    - name: Generate kubeadm config
      template:
        src: kubeadm-config.yaml.j2
        dest: /etc/kubernetes/kubeadm-config.yaml
        mode: 0644

    # Phase 4: Cluster Bootstrap
    - name: Initialize control plane
      command: kubeadm init --config /etc/kubernetes/kubeadm-config.yaml
      register: init_result
      changed_when: "'initialized' in init_result.stdout"
      args:
        creates: /etc/kubernetes/admin.conf

    # Phase 5: Client Setup
    - name: Configure kubectl access
      block:
        - name: Create .kube directory
          file:
            path: $HOME/.kube
            state: directory
            mode: 0755

        - name: Copy admin config
          copy:
            src: /etc/kubernetes/admin.conf
            dest: $HOME/.kube/config
            remote_src: yes
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: 0600

    # Phase 6: Networking
    - name: Install Flannel CNI
      command: kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
      when: init_result is changed

    # Phase 7: Cloud Integration
    - name: Configure cloud provider
      template:
        src: cloud-provider.conf.j2
        dest: /etc/kubernetes/cloud-provider.conf
        mode: 0644
      notify: restart kubelet

    # Phase 8: Security Hardening
    - name: Configure firewall
      block:
        - name: Open control plane ports
          ufw:
            rule: allow
            port: "{{ item }}"
            proto: tcp
          loop:
            - "{{ api_server_port }}"
            - "{{ etcd_client_port }}"
            - "{{ etcd_server_port }}"
            - "{{ kubelet_port }}"
            - "{{ kube_scheduler_port }}"
            - "{{ kube_controller_manager_port }}"

        - name: Set default deny policy
          ufw:
            state: enabled
            policy: deny

    # Phase 9: Worker Preparation
    - name: Generate join command
      command: kubeadm token create --print-join-command
      register: join_command
      changed_when: false
      environment:
        AWS_ROLE_ARN: "arn:aws:iam::{{ account_id }}:role/{{ role_name }}"

    - name: Distribute join command
      set_fact:
        kubeadm_join_command: "{{ join_command.stdout }}"
      delegate_to: "{{ item }}"
      delegate_facts: true
      loop: "{{ groups['worker'] }}"

    # Phase 10: Verification Suite
    - name: Validate IAM profile
      uri:
        url: http://169.254.169.254/latest/meta-data/iam/info
        return_content: yes
      register: iam_metadata
      failed_when: >
        iam_metadata.status != 200 or
        'InstanceProfileArn' not in iam_metadata.content or
        'k8s-{{ CLUSTER_ID }}-master-profile' not in iam_metadata.content

    - name: Verify OIDC discovery endpoint
      uri:
        url: "{{ OIDC_ISSUER_URL }}/.well-known/openid-configuration"
        return_content: yes
        status_code: 200
      register: oidc_discovery
      until: oidc_discovery.status == 200
      retries: 5
      delay: 10

    - name: Check API server OIDC config
      command: >
        kubectl get --raw '/.well-known/openid-configuration'
      register: api_server_oidc
      failed_when: >
        OIDC_ISSUER_URL not in api_server_oidc.stdout

    - name: Verify control plane health
      command: kubectl get --raw='/readyz?verbose'
      register: health_check
      failed_when: "'healthz check passed' not in health_check.stdout"

  handlers:
    - name: restart containerd
      service:
        name: containerd
        state: restarted

    - name: restart kubelet
      service:
        name: kubelet
        state: restarted

- name: Configure OIDC discovery
  hosts: k8_master
  become: yes
  roles:
    - role: oidc-discovery
      vars:
        domain: "{{ DOMAIN }}"
        letsencrypt_email: "{{ LETSENCRYPT_EMAIL }}"
  when: oidc_enabled | default(true)
